{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking distributed training with PyTorch DataParallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook allows to you measure the first and second epoch train time for training deep learning models on multiple GPUs using the PyTorch DataParallel strategy.\n",
    "\n",
    "This experiment varies the following:\n",
    "\n",
    "* Dataset size: MNIST digits repeated 1x, 4x, 8x = 60k, 240k, 480k training images\n",
    "* Model size: Small with 402k trainable parameters, large with 2.6m trainable parameters. Both: adam optimizer, cross entropy loss\n",
    "* Batch size: 128, 256, 512 images\n",
    "* GPUs: I used: GCP n1-highmem-2 (2 vCPUs, 13 GB memory) with {1, 2, 4} NVIDIA Tesla K80 GPUs\n",
    "\n",
    "And then it records:\n",
    "\n",
    "* First epoch train time: incurs any startup costs\n",
    "* Second epoch train time: representative of future epoch train times since training incurs same number of operations/epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from time import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch transforms.ToTensor implicitly divides pixel values by 255\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set = datasets.MNIST('data/', download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST('data/', download=True, train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Flatten()\n",
      "  (4): Linear(in_features=6272, out_features=64, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (4): ReLU()\n",
      "  (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (7): ReLU()\n",
      "  (8): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (9): Flatten()\n",
      "  (10): Linear(in_features=4608, out_features=512, bias=True)\n",
      "  (11): ReLU()\n",
      "  (12): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (13): ReLU()\n",
      "  (14): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "small_model = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d((2,2)),\n",
    "                              nn.Flatten(),\n",
    "                              nn.Linear(6272, 64),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(64, 10)\n",
    "                             )\n",
    "\n",
    "large_model = nn.Sequential(nn.Conv2d(1, 128, 3, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d((2,2)),\n",
    "                              nn.Conv2d(128, 256, 3, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d((2,2)),\n",
    "                              nn.Conv2d(256, 512, 3, padding=1),\n",
    "                              nn.ReLU(),\n",
    "                              nn.MaxPool2d((2,2)),                \n",
    "                              nn.Flatten(),\n",
    "                              nn.Linear(4608, 512),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(512, 512),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(512, 10)\n",
    "                             )\n",
    "\n",
    "model_fncs = {'small': small_model, 'large': large_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        return 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define experiment\n",
    "The experiment function takes in the three variables we are interested in: number of times to repeat the MNIST dataset, batch_size (per replica), and the number of GPUs to use. It will save the time it takes to train the first and second epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using default params for adam optimizer from tf.keras.optim\n",
    "\n",
    "def experiment(model_size, n_dataset_repeat, batch_size_per_replica, n_gpus, record_results=False):\n",
    "    batch_size = batch_size_per_replica * n_gpus\n",
    "\n",
    "    dupl_train_set = torch.utils.data.ConcatDataset([train_set for i in range(n_dataset_repeat)])\n",
    "    dupl_train_loader = torch.utils.data.DataLoader(dupl_train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = model_fncs[model_size]\n",
    "    model = nn.DataParallel(model, device_ids=[i for i in range(n_gpus)]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-07, weight_decay=0, amsgrad=False)\n",
    "    \n",
    "    epoch_times = []\n",
    "    epochs = 2\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        start = time()\n",
    "        for images, labels in dupl_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()    \n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            end = time()\n",
    "            epoch_times.append(end-start)\n",
    "            print(\"Epoch {} - Training loss: {} - Test accuracy: {} - Time(s) {}\".format(e, running_loss/len(dupl_train_loader), test(model), end-start))\n",
    "    \n",
    "    \n",
    "    if record_results:\n",
    "        with open('pytorch_results.txt', 'a') as f:\n",
    "            f.write(json.dumps(\n",
    "                        {'n_dataset_repeat': n_dataset_repeat,\n",
    "                        'batch_size': batch_size_per_replica,\n",
    "                        'n_gpus': n_gpus,\n",
    "                        'model_size': model_size,\n",
    "                        'first epoch time': epoch_times[0],\n",
    "                        'second epoch time': epoch_times[1]}) + '\\n')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: small model on dataset repeated 1x with batch size 128 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0005689484065713143 - Test accuracy: 98.68 - Time(s) 10.922572374343872\n",
      "Epoch 1 - Training loss: 2.435248299778013e-05 - Test accuracy: 98.71 - Time(s) 8.288250207901001\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: large model on dataset repeated 1x with batch size 128 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.00335146024811733 - Test accuracy: 99.18 - Time(s) 20.00277328491211\n",
      "Epoch 1 - Training loss: 0.0035170903741790754 - Test accuracy: 99.4 - Time(s) 19.96966004371643\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: small model on dataset repeated 1x with batch size 256 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0001330823421486164 - Test accuracy: 98.49 - Time(s) 7.674724817276001\n",
      "Epoch 1 - Training loss: 0.0003205880222317242 - Test accuracy: 98.63 - Time(s) 7.642316818237305\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: large model on dataset repeated 1x with batch size 256 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0010705590857315208 - Test accuracy: 99.35 - Time(s) 16.042790412902832\n",
      "Epoch 1 - Training loss: 0.0013027697193862522 - Test accuracy: 99.3 - Time(s) 15.728047132492065\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: small model on dataset repeated 1x with batch size 512 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 4.740991913052513e-05 - Test accuracy: 98.78 - Time(s) 7.320529222488403\n",
      "Epoch 1 - Training loss: 1.37649483773295e-05 - Test accuracy: 98.8 - Time(s) 7.403287649154663\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: large model on dataset repeated 1x with batch size 512 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0021932809922727554 - Test accuracy: 99.42 - Time(s) 15.040804862976074\n",
      "Epoch 1 - Training loss: 0.00044269351706982166 - Test accuracy: 99.44 - Time(s) 14.86512541770935\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: small model on dataset repeated 4x with batch size 128 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0003021658256112636 - Test accuracy: 98.78 - Time(s) 32.828110694885254\n",
      "Epoch 1 - Training loss: 4.685516451805163e-06 - Test accuracy: 98.78 - Time(s) 33.0769829750061\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: large model on dataset repeated 4x with batch size 128 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0026821501234778142 - Test accuracy: 99.41 - Time(s) 78.52960395812988\n",
      "Epoch 1 - Training loss: 1.267870652118523e-06 - Test accuracy: 99.42 - Time(s) 77.66851115226746\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: small model on dataset repeated 4x with batch size 256 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0003970881925730953 - Test accuracy: 98.74 - Time(s) 30.575629472732544\n",
      "Epoch 1 - Training loss: 6.731398958351004e-06 - Test accuracy: 98.8 - Time(s) 30.930046319961548\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: large model on dataset repeated 4x with batch size 256 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0020309610038804988 - Test accuracy: 99.33 - Time(s) 63.00147557258606\n",
      "Epoch 1 - Training loss: 0.0015534300233753372 - Test accuracy: 99.43 - Time(s) 62.92894458770752\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: small model on dataset repeated 4x with batch size 512 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.00015312570157743237 - Test accuracy: 98.61 - Time(s) 29.520257234573364\n",
      "Epoch 1 - Training loss: 8.792636176525427e-05 - Test accuracy: 98.72 - Time(s) 29.322933435440063\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: large model on dataset repeated 4x with batch size 512 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0009862095257515049 - Test accuracy: 99.52 - Time(s) 59.56726408004761\n",
      "Epoch 1 - Training loss: 2.265888008665531e-08 - Test accuracy: 99.52 - Time(s) 59.49872636795044\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: small model on dataset repeated 8x with batch size 128 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.00043857136939962706 - Test accuracy: 98.75 - Time(s) 65.73489570617676\n",
      "Epoch 1 - Training loss: 2.7844791611035666e-06 - Test accuracy: 98.77 - Time(s) 65.78944325447083\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: large model on dataset repeated 8x with batch size 128 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0026292497915196386 - Test accuracy: 99.3 - Time(s) 156.1218285560608\n",
      "Epoch 1 - Training loss: 0.0008991909714182838 - Test accuracy: 99.44 - Time(s) 152.34752106666565\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: small model on dataset repeated 8x with batch size 256 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.00031649801642574403 - Test accuracy: 98.74 - Time(s) 61.265419483184814\n",
      "Epoch 1 - Training loss: 2.803495473988942e-06 - Test accuracy: 98.74 - Time(s) 61.22239804267883\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: large model on dataset repeated 8x with batch size 256 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 1.3101067560822215e-09 - Test accuracy: 99.48 - Time(s) 126.21505188941956\n",
      "Epoch 1 - Training loss: 1.2907455724947997e-11 - Test accuracy: 99.47 - Time(s) 125.57071924209595\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: small model on dataset repeated 8x with batch size 512 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 0.0002138677608537715 - Test accuracy: 98.73 - Time(s) 59.10948944091797\n",
      "Epoch 1 - Training loss: 3.1519475791896987e-06 - Test accuracy: 98.79 - Time(s) 59.09638023376465\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: large model on dataset repeated 8x with batch size 512 on 2 gpu(s)\n",
      "Epoch 0 - Training loss: 3.971524838445537e-12 - Test accuracy: 99.47 - Time(s) 118.82844519615173\n",
      "Epoch 1 - Training loss: 3.971524838445537e-12 - Test accuracy: 99.47 - Time(s) 118.38112664222717\n"
     ]
    }
   ],
   "source": [
    "MODELS = ['small', 'large']\n",
    "DATASET_REPEATS = [1,4,8]\n",
    "BATCH_SIZES = [128, 256, 512]\n",
    "# GPU_NUMS = [i for i in range(torch.cuda.device_count()+1) if i in (1,2,4,8)]\n",
    "GPU_NUMS = [2]\n",
    "\n",
    "for d in DATASET_REPEATS:\n",
    "    for b in BATCH_SIZES:\n",
    "        for g in GPU_NUMS:\n",
    "            for m in MODELS:\n",
    "                print('\\n' + '*'*80 + '\\n')\n",
    "                print('Now training: {} model on dataset repeated {}x with batch size {} on {} gpu(s)'.format(m, d, b, g))\n",
    "                experiment(m,d,b,g, record_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"n_dataset_repeat\": 1, \"batch_size\": 128, \"n_gpus\": 1, \"first epoch time\": 9.389503240585327, \"second epoch time\": 9.451826333999634}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 128, \"n_gpus\": 1, \"first epoch time\": 26.72149634361267, \"second epoch time\": 26.690542936325073}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 256, \"n_gpus\": 1, \"first epoch time\": 8.530330896377563, \"second epoch time\": 8.714176416397095}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 256, \"n_gpus\": 1, \"first epoch time\": 24.03084683418274, \"second epoch time\": 23.860966682434082}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 512, \"n_gpus\": 1, \"first epoch time\": 8.179916381835938, \"second epoch time\": 8.380159378051758}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 512, \"n_gpus\": 1, \"first epoch time\": 23.048163652420044, \"second epoch time\": 22.975470066070557}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 128, \"n_gpus\": 1, \"first epoch time\": 37.70245623588562, \"second epoch time\": 37.85469889640808}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 128, \"n_gpus\": 1, \"first epoch time\": 107.03949117660522, \"second epoch time\": 106.68094205856323}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 256, \"n_gpus\": 1, \"first epoch time\": 34.60149145126343, \"second epoch time\": 34.88966393470764}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 256, \"n_gpus\": 1, \"first epoch time\": 95.54771018028259, \"second epoch time\": 95.95723605155945}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 512, \"n_gpus\": 1, \"first epoch time\": 33.49020981788635, \"second epoch time\": 33.64478325843811}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 512, \"n_gpus\": 1, \"first epoch time\": 91.86832690238953, \"second epoch time\": 91.72395157814026}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 128, \"n_gpus\": 1, \"first epoch time\": 75.36261296272278, \"second epoch time\": 75.59861850738525}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 128, \"n_gpus\": 1, \"first epoch time\": 212.86786031723022, \"second epoch time\": 212.98857307434082}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 256, \"n_gpus\": 1, \"first epoch time\": 69.67808842658997, \"second epoch time\": 69.92156100273132}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 256, \"n_gpus\": 1, \"first epoch time\": 191.02868175506592, \"second epoch time\": 191.1185450553894}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 512, \"n_gpus\": 1, \"first epoch time\": 66.54785442352295, \"second epoch time\": 66.78100419044495}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 512, \"n_gpus\": 1, \"first epoch time\": 182.99187564849854, \"second epoch time\": 183.13957047462463}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 128, \"n_gpus\": 2, \"first epoch time\": 10.922572374343872, \"second epoch time\": 8.288250207901001}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 128, \"n_gpus\": 2, \"first epoch time\": 20.00277328491211, \"second epoch time\": 19.96966004371643}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 256, \"n_gpus\": 2, \"first epoch time\": 7.674724817276001, \"second epoch time\": 7.642316818237305}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 256, \"n_gpus\": 2, \"first epoch time\": 16.042790412902832, \"second epoch time\": 15.728047132492065}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 512, \"n_gpus\": 2, \"first epoch time\": 7.320529222488403, \"second epoch time\": 7.403287649154663}\r\n",
      "{\"n_dataset_repeat\": 1, \"batch_size\": 512, \"n_gpus\": 2, \"first epoch time\": 15.040804862976074, \"second epoch time\": 14.86512541770935}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 128, \"n_gpus\": 2, \"first epoch time\": 32.828110694885254, \"second epoch time\": 33.0769829750061}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 128, \"n_gpus\": 2, \"first epoch time\": 78.52960395812988, \"second epoch time\": 77.66851115226746}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 256, \"n_gpus\": 2, \"first epoch time\": 30.575629472732544, \"second epoch time\": 30.930046319961548}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 256, \"n_gpus\": 2, \"first epoch time\": 63.00147557258606, \"second epoch time\": 62.92894458770752}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 512, \"n_gpus\": 2, \"first epoch time\": 29.520257234573364, \"second epoch time\": 29.322933435440063}\r\n",
      "{\"n_dataset_repeat\": 4, \"batch_size\": 512, \"n_gpus\": 2, \"first epoch time\": 59.56726408004761, \"second epoch time\": 59.49872636795044}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 128, \"n_gpus\": 2, \"first epoch time\": 65.73489570617676, \"second epoch time\": 65.78944325447083}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 128, \"n_gpus\": 2, \"first epoch time\": 156.1218285560608, \"second epoch time\": 152.34752106666565}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 256, \"n_gpus\": 2, \"first epoch time\": 61.265419483184814, \"second epoch time\": 61.22239804267883}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 256, \"n_gpus\": 2, \"first epoch time\": 126.21505188941956, \"second epoch time\": 125.57071924209595}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 512, \"n_gpus\": 2, \"first epoch time\": 59.10948944091797, \"second epoch time\": 59.09638023376465}\r\n",
      "{\"n_dataset_repeat\": 8, \"batch_size\": 512, \"n_gpus\": 2, \"first epoch time\": 118.82844519615173, \"second epoch time\": 118.38112664222717}\r\n"
     ]
    }
   ],
   "source": [
    "! cat results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
