{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "# Benchmarking distributed training with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6P32iYYV27b"
   },
   "source": [
    "Adapted from tensorflow distributed tutorial at https://www.tensorflow.org/tutorials/distribute/keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook allows to you measure the first and second epoch train time for training deep learning models on multiple GPUs using the tensorflow MirroredStrategy for keras API. \n",
    "\n",
    "This experiment varies the following:\n",
    "\n",
    "* Dataset size: MNIST digits repeated 1x, 4x, 8x = 60k, 240k, 480k training images\n",
    "* Model size: Small with 402k trainable parameters, large with 2.6m trainable parameters. Both: adam optimizer, cross entropy loss\n",
    "* Batch size: 128, 256, 512 images\n",
    "* GPUs: I used: GCP n1-highmem-2 (2 vCPUs, 13 GB memory) with {1, 2, 4} NVIDIA Tesla K80 GPUs\n",
    "\n",
    "And then it records:\n",
    "\n",
    "* First epoch train time: incurs any startup costs\n",
    "* Second epoch train time: representative of future epoch train times since training incurs same number of operations/epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dney9v7BsJij"
   },
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "r8S3ublR7Ay8"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import time\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4657818615328315539\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 35951343315859641\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4858784739591713473\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16807915064472844064\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11269973607\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 4274797351884350489\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11269973607\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 1264492189770258956\n",
      "physical_device_desc: \"device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7\"\n",
      "]\n",
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SkocY8tgRd3H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXhefksNKk2I"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(N, dataset='mnist'):\n",
    "    '''\n",
    "    Download the MNIST dataset and load it from [TensorFlow Datasets]\n",
    "    (https://www.tensorflow.org/datasets). This returns a dataset in `tf.data` format. \n",
    "    The helper function `get_dataset` also handles repeating the dataset N times \n",
    "    when we want to scale it up for the experiment.\n",
    "    '''\n",
    "    d = tfds.load(name=dataset, as_supervised=True)\n",
    "    mnist_train, mnist_test = d['train'], d['test']\n",
    "\n",
    "    for i in range(1, N):\n",
    "        d = tfds.load(name=dataset, as_supervised=True)\n",
    "        single_mnist_train, single_mnist_test = d['train'], d['test']\n",
    "\n",
    "        mnist_train = mnist_train.concatenate(single_mnist_train)\n",
    "        mnist_test = mnist_test.concatenate(single_mnist_test)\n",
    "\n",
    "    return mnist_train, mnist_test\n",
    "\n",
    "def scale(image, label):\n",
    "    '''\n",
    "    Pixel values, which are 0-255, [have to be normalized to the 0-1 range]\n",
    "    (https://en.wikipedia.org/wiki/Feature_scaling). Define this scale in a function.\n",
    "    '''\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "\n",
    "    return image, label\n",
    "\n",
    "def define_small_model():\n",
    "    model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(64, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def define_large_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(128, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(256, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(512, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def decay(epoch):\n",
    "    '''\n",
    "    Function for decaying the learning rate.\n",
    "    You can define any decay function you need.\n",
    "    '''\n",
    "    if epoch < 3:\n",
    "        return 1e-3\n",
    "    elif epoch >= 3 and epoch < 7:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrjVhv-eKuHD"
   },
   "source": [
    "## Define experiment\n",
    "The experiment function takes in the three variables we are interested in: number of times to repeat the MNIST dataset, batch_size (per replica), and the number of GPUs to use. It will save the time it takes to train the first and second epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4j0tdf4YB3X9"
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "\n",
    "def experiment(define_model_fnc, n_dataset_repeat, batch_size_per_replica, n_gpus, record_results=False):    \n",
    "    \n",
    "    # define data parallel strategy if using more than one gpu\n",
    "    if n_gpus > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:{}\".format(i) for i in range(n_gpus)])\n",
    "        print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "    # set up batch size var. depends on how many gpus are being used\n",
    "    BUFFER_SIZE = 10000\n",
    "\n",
    "    if n_gpus > 1:\n",
    "        BATCH_SIZE = batch_size_per_replica * strategy.num_replicas_in_sync\n",
    "    else:\n",
    "        BATCH_SIZE = batch_size_per_replica\n",
    "    \n",
    "    # download and process dataset\n",
    "    mnist_train, mnist_test = get_dataset(n_dataset_repeat, 'mnist')\n",
    "    train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\n",
    "    \n",
    "    # Create and compile the Keras model in the context of `strategy.scope`.\n",
    "    if n_gpus>1:\n",
    "        with strategy.scope():\n",
    "            model = define_model_fnc()\n",
    "    else:\n",
    "        model = define_model_fnc()\n",
    "    \n",
    "    # define callback that will record the epoch train time\n",
    "    epoch_times = []\n",
    "    class timecallback(tf.keras.callbacks.Callback):            \n",
    "        def on_epoch_begin(self,epoch,logs={}):\n",
    "            self.starttime = time.clock()\n",
    "            \n",
    "        def on_epoch_end(self,epoch,logs = {}):\n",
    "            epoch_times.append(time.clock() - self.starttime)\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "        tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "        timecallback(),\n",
    "    ]\n",
    "    \n",
    "    model.fit(train_dataset, epochs=2, callbacks=callbacks)\n",
    "    \n",
    "    if record_results:\n",
    "        with open('tensorflow_results.txt', 'a') as f:\n",
    "            f.write(json.dumps(\n",
    "                        {'n_dataset_repeat': n_dataset_repeat,\n",
    "                        'batch_size': batch_size_per_replica,\n",
    "                        'n_gpus': n_gpus,\n",
    "                        'first epoch time': epoch_times[0],\n",
    "                        'second epoch time': epoch_times[1]}) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 1x with batch size 128 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "469/469 [==============================] - 19s 42ms/step - loss: 0.2707 - accuracy: 0.9246\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 3s 7ms/step - loss: 0.0840 - accuracy: 0.9757\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 1x with batch size 128 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "469/469 [==============================] - 32s 68ms/step - loss: 0.2091 - accuracy: 0.9338\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 15s 32ms/step - loss: 0.0554 - accuracy: 0.9834\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 1x with batch size 128 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 23s 99ms/step - loss: 0.3152 - accuracy: 0.9128\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 3s 12ms/step - loss: 0.0925 - accuracy: 0.9736\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 1x with batch size 128 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 26s 111ms/step - loss: 0.2721 - accuracy: 0.9110\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 9s 38ms/step - loss: 0.0629 - accuracy: 0.9809\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 1x with batch size 256 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "235/235 [==============================] - 19s 80ms/step - loss: 0.3417 - accuracy: 0.9057\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 2s 9ms/step - loss: 0.1086 - accuracy: 0.9696\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 1x with batch size 256 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "235/235 [==============================] - 28s 121ms/step - loss: 0.2619 - accuracy: 0.9147\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 13s 55ms/step - loss: 0.0643 - accuracy: 0.9801\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 1x with batch size 256 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 21s 181ms/step - loss: 0.4658 - accuracy: 0.8726\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 2s 15ms/step - loss: 0.1684 - accuracy: 0.9524\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 1x with batch size 256 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 12 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 22s 190ms/step - loss: 0.4210 - accuracy: 0.8629\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 7s 62ms/step - loss: 0.0780 - accuracy: 0.9758\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 1x with batch size 512 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 19s 159ms/step - loss: 0.4395 - accuracy: 0.8834\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 2s 14ms/step - loss: 0.1445 - accuracy: 0.9586\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 1x with batch size 512 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "118/118 [==============================] - 28s 236ms/step - loss: 0.3716 - accuracy: 0.8796\n",
      "Epoch 2/2\n",
      "118/118 [==============================] - 13s 108ms/step - loss: 0.0793 - accuracy: 0.9765\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 1x with batch size 512 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 6s 6s/step - loss: 2.2956 - accuracy: 0.1123WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.276176). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.276176). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 20s 336ms/step - loss: 0.6468 - accuracy: 0.8382\n",
      "Epoch 2/2\n",
      "59/59 [==============================] - 1s 20ms/step - loss: 0.2018 - accuracy: 0.9412\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 1x with batch size 512 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "59/59 [==============================] - 23s 389ms/step - loss: 0.6064 - accuracy: 0.8087\n",
      "Epoch 2/2\n",
      "59/59 [==============================] - 7s 118ms/step - loss: 0.1090 - accuracy: 0.9674\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 4x with batch size 128 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 78s 42ms/step - loss: 0.1050 - accuracy: 0.9698\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0248 - accuracy: 0.9924\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 4x with batch size 128 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 126s 67ms/step - loss: 0.0823 - accuracy: 0.9740\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0189 - accuracy: 0.9942\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 4x with batch size 128 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.1451 - accuracy: 0.9591\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.0357 - accuracy: 0.9897\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 4x with batch size 128 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 91s 97ms/step - loss: 0.0975 - accuracy: 0.9682\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 36s 38ms/step - loss: 0.0183 - accuracy: 0.9942\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 4x with batch size 256 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 76s 81ms/step - loss: 0.1344 - accuracy: 0.9624\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 8s 9ms/step - loss: 0.0335 - accuracy: 0.9903\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 4x with batch size 256 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 109s 116ms/step - loss: 0.1019 - accuracy: 0.9678\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 52s 55ms/step - loss: 0.0195 - accuracy: 0.9937\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 4x with batch size 256 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "469/469 [==============================] - 77s 164ms/step - loss: 0.1909 - accuracy: 0.9488\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 7s 14ms/step - loss: 0.0465 - accuracy: 0.9870\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 4x with batch size 256 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "469/469 [==============================] - 82s 175ms/step - loss: 0.1425 - accuracy: 0.9548\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 29s 62ms/step - loss: 0.0234 - accuracy: 0.9925\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 4x with batch size 512 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "469/469 [==============================] - 73s 155ms/step - loss: 0.1979 - accuracy: 0.9464\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 6s 13ms/step - loss: 0.0486 - accuracy: 0.9862\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 4x with batch size 512 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "469/469 [==============================] - 104s 223ms/step - loss: 0.1322 - accuracy: 0.9575\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 51s 108ms/step - loss: 0.0210 - accuracy: 0.9934\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 4x with batch size 512 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "      1/Unknown - 5s 5s/step - loss: 2.3313 - accuracy: 0.1328WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.367691). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.367691). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 70s 298ms/step - loss: 0.2821 - accuracy: 0.9223\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.0766 - accuracy: 0.9786\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 4x with batch size 512 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "      1/Unknown - 7s 7s/step - loss: 2.3032 - accuracy: 0.0947WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.433241). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.433241). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235/235 [==============================] - 80s 339ms/step - loss: 0.2045 - accuracy: 0.9336\n",
      "Epoch 2/2\n",
      "235/235 [==============================] - 27s 117ms/step - loss: 0.0325 - accuracy: 0.9900\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 8x with batch size 128 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 160s 43ms/step - loss: 0.0647 - accuracy: 0.9814\n",
      "Epoch 2/2\n",
      "3750/3750 [==============================] - 26s 7ms/step - loss: 0.0079 - accuracy: 0.9977\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 8x with batch size 128 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "3750/3750 [==============================] - 246s 66ms/step - loss: 0.0494 - accuracy: 0.9844\n",
      "Epoch 2/2\n",
      "3750/3750 [==============================] - 119s 32ms/step - loss: 0.0100 - accuracy: 0.9970\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 8x with batch size 128 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "      1/Unknown - 5s 5s/step - loss: 2.3144 - accuracy: 0.0898WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116285). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116285). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 153s 82ms/step - loss: 0.0925 - accuracy: 0.9733\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 22s 11ms/step - loss: 0.0134 - accuracy: 0.9964\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 8x with batch size 128 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "      1/Unknown - 7s 7s/step - loss: 2.3044 - accuracy: 0.0703WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.124822). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.124822). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 178s 95ms/step - loss: 0.0600 - accuracy: 0.9811\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0087 - accuracy: 0.9972\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 8x with batch size 256 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 151s 81ms/step - loss: 0.0865 - accuracy: 0.9754\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0123 - accuracy: 0.9967\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 8x with batch size 256 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 216s 115ms/step - loss: 0.0601 - accuracy: 0.9809\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 103s 55ms/step - loss: 0.0088 - accuracy: 0.9972\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 8x with batch size 256 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "      1/Unknown - 7s 7s/step - loss: 2.3182 - accuracy: 0.0723WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225246). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.225246). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 153s 163ms/step - loss: 0.1229 - accuracy: 0.9657\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 14s 15ms/step - loss: 0.0251 - accuracy: 0.9930\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 8x with batch size 256 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "      1/Unknown - 6s 6s/step - loss: 2.3014 - accuracy: 0.1328WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.208290). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.208290). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 162s 173ms/step - loss: 0.0812 - accuracy: 0.9742\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 59s 62ms/step - loss: 0.0089 - accuracy: 0.9971\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 8x with batch size 512 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 144s 153ms/step - loss: 0.1197 - accuracy: 0.9677\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 12s 13ms/step - loss: 0.0223 - accuracy: 0.9941\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 8x with batch size 512 on 1 gpu(s)\n",
      "Epoch 1/2\n",
      "938/938 [==============================] - 208s 222ms/step - loss: 0.0804 - accuracy: 0.9742\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 100s 107ms/step - loss: 0.0087 - accuracy: 0.9971\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_small_model on dataset repeated 8x with batch size 512 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "      1/Unknown - 7s 7s/step - loss: 2.3062 - accuracy: 0.0684WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.428031). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.428031). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 140s 298ms/step - loss: 0.1861 - accuracy: 0.9507\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 8s 18ms/step - loss: 0.0365 - accuracy: 0.9899\n",
      "\n",
      "********************************************************************************\n",
      "\n",
      "Now training: define_large_model on dataset repeated 8x with batch size 512 on 2 gpu(s)\n",
      "Number of devices: 2\n",
      "Epoch 1/2\n",
      "469/469 [==============================] - 154s 328ms/step - loss: 0.1224 - accuracy: 0.9617\n",
      "Epoch 2/2\n",
      "469/469 [==============================] - 55s 116ms/step - loss: 0.0129 - accuracy: 0.9959\n"
     ]
    }
   ],
   "source": [
    "MODELS = [define_small_model, define_large_model]\n",
    "DATASET_REPEATS = [1,4,8]\n",
    "BATCH_SIZES = [128, 256, 512]\n",
    "GPU_NUMS = [i for i in range(len(tf.config.experimental.list_physical_devices('GPU'))+1) if i in (1,2,4,8)]\n",
    "\n",
    "for d in DATASET_REPEATS:\n",
    "    for b in BATCH_SIZES:\n",
    "        for g in GPU_NUMS:\n",
    "            for m in MODELS:\n",
    "                print('\\n' + '*'*80 + '\\n')\n",
    "                print('Now training: {} on dataset repeated {}x with batch size {} on {} gpu(s)'.format(m.__name__, d, b, g))\n",
    "                experiment(m,d,b,g, record_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"n_gpus\": 1, \"second epoch time\": 3.526135999999994, \"batch_size\": 128, \"n_dataset_repeat\": 1, \"first epoch time\": 35.357262}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 12.653536000000003, \"batch_size\": 128, \"n_dataset_repeat\": 1, \"first epoch time\": 46.01204200000001}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 4.091595000000012, \"batch_size\": 128, \"n_dataset_repeat\": 1, \"first epoch time\": 38.85839100000001}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 7.8085109999999815, \"batch_size\": 128, \"n_dataset_repeat\": 1, \"first epoch time\": 43.34130999999999}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 2.1493070000000216, \"batch_size\": 256, \"n_dataset_repeat\": 1, \"first epoch time\": 34.963386000000014}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 10.277569000000028, \"batch_size\": 256, \"n_dataset_repeat\": 1, \"first epoch time\": 42.76192099999997}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 2.3124480000000176, \"batch_size\": 256, \"n_dataset_repeat\": 1, \"first epoch time\": 36.45837400000005}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 4.762603999999953, \"batch_size\": 256, \"n_dataset_repeat\": 1, \"first epoch time\": 40.388553}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 1.5289939999999547, \"batch_size\": 512, \"n_dataset_repeat\": 1, \"first epoch time\": 34.96796699999999}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 9.619731999999999, \"batch_size\": 512, \"n_dataset_repeat\": 1, \"first epoch time\": 42.06703199999998}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 1.3742380000001049, \"batch_size\": 512, \"n_dataset_repeat\": 1, \"first epoch time\": 35.37769199999997}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 3.6254279999999426, \"batch_size\": 512, \"n_dataset_repeat\": 1, \"first epoch time\": 40.59063000000003}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 13.964889999999968, \"batch_size\": 128, \"n_dataset_repeat\": 4, \"first epoch time\": 143.555698}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 50.34216200000003, \"batch_size\": 128, \"n_dataset_repeat\": 4, \"first epoch time\": 185.672146}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 16.107735000000048, \"batch_size\": 128, \"n_dataset_repeat\": 4, \"first epoch time\": 146.33646899999997}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 30.455420000000004, \"batch_size\": 128, \"n_dataset_repeat\": 4, \"first epoch time\": 172.609238}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 8.385099999999966, \"batch_size\": 256, \"n_dataset_repeat\": 4, \"first epoch time\": 141.09680600000002}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 40.978718999999955, \"batch_size\": 256, \"n_dataset_repeat\": 4, \"first epoch time\": 168.92927800000007}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 9.179738000000043, \"batch_size\": 256, \"n_dataset_repeat\": 4, \"first epoch time\": 147.8159129999999}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 18.776170000000093, \"batch_size\": 256, \"n_dataset_repeat\": 4, \"first epoch time\": 157.87104900000008}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 5.574841000000106, \"batch_size\": 512, \"n_dataset_repeat\": 4, \"first epoch time\": 136.0877889999997}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 38.307256999999936, \"batch_size\": 512, \"n_dataset_repeat\": 4, \"first epoch time\": 164.93782999999985}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 5.315678999999818, \"batch_size\": 512, \"n_dataset_repeat\": 4, \"first epoch time\": 134.46636799999987}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 14.420806999999968, \"batch_size\": 512, \"n_dataset_repeat\": 4, \"first epoch time\": 149.10796600000003}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 28.09298099999978, \"batch_size\": 128, \"n_dataset_repeat\": 8, \"first epoch time\": 293.6904440000003}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 100.40474699999959, \"batch_size\": 128, \"n_dataset_repeat\": 8, \"first epoch time\": 372.5076650000001}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 32.7153959999996, \"batch_size\": 128, \"n_dataset_repeat\": 8, \"first epoch time\": 297.9061849999998}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 60.628154999999424, \"batch_size\": 128, \"n_dataset_repeat\": 8, \"first epoch time\": 341.95641000000023}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 16.49770400000034, \"batch_size\": 256, \"n_dataset_repeat\": 8, \"first epoch time\": 281.0998120000004}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 81.60598199999913, \"batch_size\": 256, \"n_dataset_repeat\": 8, \"first epoch time\": 336.9119899999996}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 18.693081999999777, \"batch_size\": 256, \"n_dataset_repeat\": 8, \"first epoch time\": 294.5087289999992}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 37.51893699999982, \"batch_size\": 256, \"n_dataset_repeat\": 8, \"first epoch time\": 312.59651099999974}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 11.095134000000144, \"batch_size\": 512, \"n_dataset_repeat\": 8, \"first epoch time\": 267.36950999999954}\r\n",
      "{\"n_gpus\": 1, \"second epoch time\": 76.10208700000021, \"batch_size\": 512, \"n_dataset_repeat\": 8, \"first epoch time\": 325.0966699999999}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 10.383460999999443, \"batch_size\": 512, \"n_dataset_repeat\": 8, \"first epoch time\": 270.12654499999917}\r\n",
      "{\"n_gpus\": 2, \"second epoch time\": 28.882407000000057, \"batch_size\": 512, \"n_dataset_repeat\": 8, \"first epoch time\": 296.4192949999997}\r\n"
     ]
    }
   ],
   "source": [
    "! cat results.txt"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keras.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
